{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"colab_button\">\n",
    "  <h1>Quick tour</h1>\n",
    "  <a target=\"_blank\" href=\"https://colab.research.google.com/github/mithril-security/blindllama/blob/main/docs/docs/getting-started/quick-tour.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>\n",
    "\n",
    "## Introduction\n",
    "________________________________________________________\n",
    "\n",
    "BlindLlama provides Confidential & transparent APIs for querying open-source models, which:\n",
    "\n",
    "1. **Serves models in a Confidential & transparent hardened environment**. Even our admins cannot access or view any user data!\n",
    "\n",
    "2. **Provides robust technical proofs** that users are communicating with **an authentic BlindLlama server deployed within a hardened environment**. This means users have technical guarantees that we cannot access or use their data!\n",
    "\n",
    "We provide a Python package which you can easily download and use to query our models. Currently, we serve the [Llama2 70b](https://huggingface.co/meta-llama/Llama-2-70b) model, but we will serve more popular open-source models in the near future.\n",
    "\n",
    "> Llama2 is is a text-generation LLM (large language model) that can be queried in a similar way to OpenAI's ChatGPT.\n",
    "\n",
    "<div class=\"alert\"><span class=\"closebtn\" onclick=\"this.parentElement.style.display='none';\">&times;</span>\n",
    "⚠️ This quick tour uses the <b>alpha</b> version of BlindLlama which gives enables you to test our solution and learn more about it.<br><br>\n",
    "It does not yet have the full security features.\n",
    "Do not test our API with confidential information... yet!<br><br>\n",
    "You can follow our progress towards the next <b>beta</b> and <b>audit-ready</b> versions of BlindLLama on our \n",
    "<a href=\"https://mithril-security.notion.site/BlindLlama-roadmap-d55883a04be446e49e01ee884c203c26\">roadmap</a>!\n",
    "</div>\n",
    "\n",
    "Let's now take a look at how you can query Llama2 with BlindLlama. \n",
    "\n",
    "You can follow along in your own environment or online using our [Google Colab notebook](https://colab.research.google.com/github/mithril-security/blindllama/blob/main/docs/docs/getting-started/quick-tour.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your access token\n",
    "\n",
    "Before you can use our API, you will need to get your personal access token from [Mithril Cloud](https://cloud.mithrilsecurity.io/).\n",
    "\n",
    "> You can get started with our API for free, but note that the number of free queries per user is capped."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Next, you'll need to install:\n",
    "- the `blind_llama` Python client\n",
    "- `tpm2-tools`, this library is used by the client to verify the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blind_llama\n",
    "!apt install tpm2-tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the model\n",
    "Now you're ready to start playing with our privacy-friendly Llama 2 model!\n",
    "\n",
    "First of all, you'll need to import the `blind_llama` package and then copy and paste your API key into the corresponding `API_KEY` variable in order to be able to use our API.\n",
    "\n",
    "BlindLlama's client SDK is based on that of `openai` to facilitate uptake for end users are already familiar with their SDK.\n",
    "\n",
    "Our querying method `completion.create()` accepts three options `model`, `prompt` and `temperature` of which only `prompt` is compulsory.:\n",
    "\n",
    "- The `prompt` option is a string input containing your query input text. Feel free to modify the `prompt` option below to test the API with new prompts!\n",
    "\n",
    "- The `model` option allows you to select the model you wish to use and is set to Llama-2-70b by default. We currently only support this model, but will add more models in the near future.\n",
    "\n",
    "- The `temperature` is the sampling temperature and should be a value between 0 and 1. The higher the value, the more random the output will be. The lower the value, the more deterministic it will be. The `temperature` option is set to 0 by default. This will make the model use log probability to automatically increase the temperature until certain thresholds are hit.\n",
    "\n",
    "The `completion.create()` method returns the response of the model as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/BlindLlama/env/lib/python3.10/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aicert_worker'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/home/laura/BlindLlama/client/blind_llama/completion.py:216: UserWarning: The quote from the TPM is not endorsed by the Cloud provider for the alpha version of BlindLlama v0.1. For more information look at https://github.com/mithril-security/blind_llama\n",
      "  warnings.warn(f\"The quote from the TPM is not endorsed by the Cloud provider for the alpha version of BlindLlama v0.1. For more information look at https://github.com/mithril-security/blind_llama\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "in 30 words or less.\n",
      "\n",
      "Python is a versatile, high-level programming language that emphasizes readability and ease of use, featuring a minimalist syntax and flexible built-in data structures.\n"
     ]
    }
   ],
   "source": [
    "import blind_llama as openai\n",
    "\n",
    "# set your Mithril Cloud API key\n",
    "openai.api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# query model and save output in response string\n",
    "response = openai.completion.create(\n",
    "  model=\"meta-llama/Llama-2-70b-chat-hf\", # set model with HuggingFace model ID\n",
    "  prompt=\"Describe the Python programming language\",\n",
    "  temperature=0.7, # set sampling temperature to generate relatively random output\n",
    ")\n",
    "print(f\"Result:\\n{response}\") # print out response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security\n",
    "________________________________________________________\n",
    "\n",
    "While on the face of it, the API appears to works just like a regular AI API, BlindLlama is doing a lot under the hood to make sure user data remains confidential!\n",
    "\n",
    "When you connect to the BlindLlama server, the client will:\n",
    "- Check that it is talking to an authentic BlindLlama server, through [attested TLS](https://blindllama.mithrilsecurity.io/en/latest/docs/concepts/attested-tls/)\n",
    "- Check that the server is [serving the expected code](https://blindllama.mithrilsecurity.io/en/latest/docs/concepts/TPMs/) and is deployed withing [a hardened Confidential & transparent environment](https://blindllama.mithrilsecurity.io/en/latest/docs/concepts/hardened-systems/)\n",
    "\n",
    "If either of these checks fail, you will see an error and will be unable to connect to the server!\n",
    "\n",
    "> ⚠️ Note that some security features will not be implemented until the beta or 1.0 launch dates. You can check out our progress on our [roadmap](https://mithril-security.notion.site/BlindLlama-roadmap-d55883a04be446e49e01ee884c203c26)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out our overview of how we make our APIs confidential [in the next section](https://blindllama.mithrilsecurity.io/en/latest/docs/getting-started/how-we-achieve-zero-trust/) and learn more about the underlining key concepts in our [concept guide](https://blindllama.mithrilsecurity.io/en/latest/docs/concepts/overview/).\n",
    "\n",
    "We also created the BlindLlama whitepaper to cover the security features behind BlindLLama in greater detail. You can read or download the whitepaper [here](https://github.com/mithril-security/blind_llama/tree/main/docs/docs/whitepaper/blind_llama_whitepaper.pdf)!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
